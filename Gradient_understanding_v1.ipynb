{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl+Mpi9daI16BARRiwE1We",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malcolmlett/ml-learning/blob/main/Gradient_understanding_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For a blog post on gradients within deep neural networks"
      ],
      "metadata": {
        "id": "6q5FH7m0SaD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CvqlK-DuSVwu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working it out\n",
        "First a set of experiments to prove the basics.\n"
      ],
      "metadata": {
        "id": "LzG1nlLrmlxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extremely simple model - with overly simple loss function\n",
        "# conclusions:\n",
        "#  - dY/dW is proportional to n\n",
        "#  - dJ/dW is averaged across all n if that's was the loss function does, however\n",
        "#  - dJ/dW will be summed across n, or any other operation, depending how the loss is computed.\n",
        "W = tf.Variable(tf.ones(shape=(3,3)), dtype=tf.float32)\n",
        "X = tf.ones(shape=(15,3))\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  Y_pred = tf.matmul(X, W)\n",
        "  loss1 = tf.reduce_mean(Y_pred)\n",
        "  loss2 = tf.reduce_sum(Y_pred)\n",
        "\n",
        "print(f\"dY/dW: {tape.gradient(Y_pred, W)}\")\n",
        "print(f\"dJ1/dW: {tape.gradient(loss1, W)}\")\n",
        "print(f\"dJ2/dW: {tape.gradient(loss2, W)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_7c3cxjSh_F",
        "outputId": "54f20574-8cfa-43b6-d420-c24e475fb911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dY/dW: [[15. 15. 15.]\n",
            " [15. 15. 15.]\n",
            " [15. 15. 15.]]\n",
            "dJ1/dW: [[0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]]\n",
            "dJ2/dW: [[15. 15. 15.]\n",
            " [15. 15. 15.]\n",
            " [15. 15. 15.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extremely simple model - with more realistic loss function\n",
        "# conclusions:\n",
        "#  - dY/dW is proportional to n\n",
        "#  - dJ/dW is averaged across all n if that's was the loss function does, however\n",
        "#  - dJ/dW will be summed across n, or any other operation, depending how the loss is computed.\n",
        "W = tf.Variable(tf.ones(shape=(3,3)), dtype=tf.float32)\n",
        "X = tf.ones(shape=(15,3))\n",
        "Y = tf.ones(shape=(15,3))\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  Y_pred = tf.matmul(X, W)\n",
        "  loss1 = tf.reduce_mean(0.5*(Y_pred - Y)**2)    # reflects the common MSE loss\n",
        "  loss2 = tf.reduce_sum(0.5*(Y_pred - Y)**2)     # for comparison if we remove the 1/n from the front\n",
        "\n",
        "# J = 1/n * sum(1/2 * (Y_pred - Y)**2)         [n = batch size, d = depth]\n",
        "# dJ/dW = dJ/dY . dY/dW\n",
        "# dJ/dY = 1/n * sum(Y_pred - Y)\n",
        "\n",
        "print(f\"X:{X[:5,:]}\")\n",
        "print(f\"Y_pred:{Y_pred[:5,:]}\")\n",
        "print(f\"error:{(Y_pred - Y)[:5,:]}\")\n",
        "print(f\"loss1:{loss1}\")\n",
        "print(f\"loss2:{loss2}\")\n",
        "print(\"---\")\n",
        "print(f\"dJ/dW = dJ/dY . dY/dW\")\n",
        "print(f\"dJ1/dY: {tape.gradient(loss1, Y_pred)[:5,:]}\")  # just first 5 rows out of 15 (all same), note: 0.04444445 = 1/15*1/3*2\n",
        "print(f\"dJ2/dY: {tape.gradient(loss2, Y_pred)[:5,:]}\")  # just first 5 rows out of 15 (all same)\n",
        "print(f\"dY/dW: {tape.gradient(Y_pred, W)}\")\n",
        "print(f\"dJ1/dW: {tape.gradient(loss1, W)}\")\n",
        "print(f\"dJ2/dW: {tape.gradient(loss2, W)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkrWai4T_Xgl",
        "outputId": "9da6f642-59b7-4368-e997-6c3cf13c0154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:[[1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "Y_pred:[[3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]]\n",
            "error:[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "loss1:2.0\n",
            "loss2:90.0\n",
            "---\n",
            "dJ/dW = dJ/dY . dY/dW\n",
            "dJ1/dY: [[0.04444445 0.04444445 0.04444445]\n",
            " [0.04444445 0.04444445 0.04444445]\n",
            " [0.04444445 0.04444445 0.04444445]\n",
            " [0.04444445 0.04444445 0.04444445]\n",
            " [0.04444445 0.04444445 0.04444445]]\n",
            "dJ2/dY: [[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "dY/dW: [[15. 15. 15.]\n",
            " [15. 15. 15.]\n",
            " [15. 15. 15.]]\n",
            "dJ1/dW: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ2/dW: [[30. 30. 30.]\n",
            " [30. 30. 30.]\n",
            " [30. 30. 30.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How do the out-of-box loss functions work with multi-dimensional outputs?\n",
        "# conclusions:\n",
        "#  - out of box MSE loss function takes the mean across the feature dimensions always\n",
        "#  - out of box MSE loss function usually takes the mean across the sample dimenion too (reduction = default = 'sum_over_batch_size')\n",
        "#     but it can alternatively take the sum, or just emit all individual values\n",
        "#\n",
        "# In other words, for a normal application using MSE, the loss function is computed as follows:\n",
        "#   J = 1/n * 1/d * sum(over batch: 1/2 * sum(over depth: (Y_pred - Y)**2))         [n = batch size, d = depth]\n",
        "#   dJ/dW = dJ/dY . dY/dW\n",
        "#   dJ/dY = 1/n * 1/d * sum_b(sum_d(Y_pred - Y)) = mean(Y_pred - Y, axis=all)\n",
        "\n",
        "W = tf.Variable(tf.ones(shape=(3,3)), dtype=tf.float32)\n",
        "X = tf.ones(shape=(15,3))\n",
        "Y = tf.ones(shape=(15,3))\n",
        "Y_pred = tf.zeros(shape=(15,3))\n",
        "print(f\"Loss from Y_pred shape {Y_pred.shape}: {tf.keras.losses.MeanSquaredError()(Y, Y_pred)}  (standard MSE, reduction = default = 'sum_over_batch_size')\")\n",
        "print(f\"Loss from Y_pred shape {Y_pred.shape}: {tf.keras.losses.MeanSquaredError(reduction='sum')(Y, Y_pred)} (reduction = 'sum')\")\n",
        "print(f\"Loss from Y_pred shape {Y_pred.shape}: {tf.keras.losses.MeanSquaredError(reduction=None)(Y, Y_pred)} (reduction = None)\")\n",
        "\n",
        "print(f\"---\")\n",
        "\n",
        "Y_pred = np.zeros(shape=(15,3))\n",
        "Y_pred[:,2] = 1\n",
        "print(f\"Loss from Y_pred shape {Y_pred.shape}: {tf.keras.losses.MeanSquaredError()(Y, Y_pred)}  (standard MSE, reduction = default = 'sum_over_batch_size')\")\n",
        "print(f\"Loss from Y_pred shape {Y_pred.shape}: {tf.keras.losses.MeanSquaredError(reduction='sum')(Y, Y_pred)} (reduction = 'sum')\")\n",
        "print(f\"Loss from Y_pred shape {Y_pred.shape}: {tf.keras.losses.MeanSquaredError(reduction=None)(Y, Y_pred)} (reduction = None)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lejiAIiQHU03",
        "outputId": "6f99033b-c7c6-4a87-e7cf-d0845c3aa014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss from Y_pred shape (15, 3): 1.0  (standard MSE, reduction = default = 'sum_over_batch_size')\n",
            "Loss from Y_pred shape (15, 3): 15.0 (reduction = 'sum')\n",
            "Loss from Y_pred shape (15, 3): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] (reduction = None)\n",
            "---\n",
            "Loss from Y_pred shape (15, 3): 0.6666666865348816  (standard MSE, reduction = default = 'sum_over_batch_size')\n",
            "Loss from Y_pred shape (15, 3): 10.0 (reduction = 'sum')\n",
            "Loss from Y_pred shape (15, 3): [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667\n",
            " 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667\n",
            " 0.6666667] (reduction = None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Same simple model as above now using out-of-box loss function\n",
        "# conclusions:\n",
        "#  We got almost exactly the same thing as when doing it manually. The only difference is the '1/2' factor. Yay.\n",
        "W = tf.Variable(tf.ones(shape=(3,3)), dtype=tf.float32)\n",
        "X = tf.ones(shape=(15,3))\n",
        "Y = tf.ones(shape=(15,3))\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()  # works the same as my tf.reduce_mean(0.5*(Y_pred - Y)**2) from above, BUT without the '1/2' factor\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  Y_pred = tf.matmul(X, W)\n",
        "  loss = loss_fn(Y, Y_pred)\n",
        "\n",
        "print(f\"X:{X[:5,:]}\")\n",
        "print(f\"Y_pred:{Y_pred[:5,:]}\")\n",
        "print(f\"error:{(Y_pred - Y)[:5,:]}\")\n",
        "print(f\"loss:{loss}\")\n",
        "print(\"---\")\n",
        "print(f\"dJ/dW = dJ/dY . dY/dW\")\n",
        "print(f\"dJ/dY: {tape.gradient(loss, Y_pred)[:5,:]}\")  # just first 5 rows out of 15 (all same), note: 0.0888889 = 1/15*1/3*2*2\n",
        "print(f\"dY/dW: {tape.gradient(Y_pred, W)}\")\n",
        "print(f\"dJ/dW: {tape.gradient(loss, W)}\")\n",
        "print(\"---\")\n",
        "print(\"jacobian:\")\n",
        "print(f\"dJ/dY: {tape.jacobian(loss, Y_pred).shape} - {tape.jacobian(loss, Y_pred)[:5,:]}\")  # just first 5 rows out of 15 (all same), note: 0.0888889 = 1/15*1/3*2*2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beu12tqWPoUl",
        "outputId": "719844c0-60a3-4213-e931-13abbfefca01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:[[1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "Y_pred:[[3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]]\n",
            "error:[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "loss:4.0\n",
            "---\n",
            "dJ/dW = dJ/dY . dY/dW\n",
            "dJ/dY: [[0.08888889 0.08888889 0.08888889]\n",
            " [0.08888889 0.08888889 0.08888889]\n",
            " [0.08888889 0.08888889 0.08888889]\n",
            " [0.08888889 0.08888889 0.08888889]\n",
            " [0.08888889 0.08888889 0.08888889]]\n",
            "dY/dW: [[15. 15. 15.]\n",
            " [15. 15. 15.]\n",
            " [15. 15. 15.]]\n",
            "dJ/dW: [[1.3333334 1.3333334 1.3333334]\n",
            " [1.3333334 1.3333334 1.3333334]\n",
            " [1.3333334 1.3333334 1.3333334]]\n",
            "---\n",
            "jacobian:\n",
            "dJ/dY: (15, 3) - [[0.0888889 0.0888889 0.0888889]\n",
            " [0.0888889 0.0888889 0.0888889]\n",
            " [0.0888889 0.0888889 0.0888889]\n",
            " [0.0888889 0.0888889 0.0888889]\n",
            " [0.0888889 0.0888889 0.0888889]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Same as above but with random values\n",
        "# Expect to see that dY/dW has each column the same value\n",
        "W = tf.Variable(tf.random.normal(shape=(3,3)), dtype=tf.float32)\n",
        "X = tf.random.normal(shape=(15,3))\n",
        "Y = tf.random.normal(shape=(15,3))\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()  # works the same as my tf.reduce_mean(0.5*(Y_pred - Y)**2) from above, BUT without the '1/2' factor\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  Y_pred = tf.matmul(X, W)\n",
        "  loss = loss_fn(Y, Y_pred)\n",
        "\n",
        "print(f\"X:{X[:5,:]}\")\n",
        "print(f\"Y_pred:{Y_pred[:5,:]}\")\n",
        "print(f\"error:{(Y_pred - Y)[:5,:]}\")\n",
        "print(f\"loss:{loss}\")\n",
        "print(\"---\")\n",
        "print(f\"dJ/dW = dJ/dY . dY/dW\")\n",
        "print(f\"dJ/dY: {tape.gradient(loss, Y_pred)[:5,:]}\")  # just first 5 rows out of 15 (all same), note: 0.0888889 = 1/15*1/3*2*2\n",
        "print(f\"dY/dW: {tape.gradient(Y_pred, W)}\")\n",
        "print(f\"dJ/dW: {tape.gradient(loss, W)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JuEMWb_ZFQi",
        "outputId": "188c3442-8bb6-4b6f-9fa9-abb766d03e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:[[-0.43365982  0.5280366   1.6713797 ]\n",
            " [ 0.7201729   0.8335855   0.8018656 ]\n",
            " [-0.47507477 -0.8587969   0.9780858 ]\n",
            " [ 1.2738467  -0.4910632   1.3990924 ]\n",
            " [-0.23144156  0.04747184 -1.6782101 ]]\n",
            "Y_pred:[[ 0.5941853  -1.8028502   0.59811175]\n",
            " [ 0.09435182 -0.742683    1.1430625 ]\n",
            " [-0.05283837 -0.6590447  -2.2933812 ]\n",
            " [-0.6865124  -0.48568112 -2.6250665 ]\n",
            " [-0.02870353  1.3116801   1.1409246 ]]\n",
            "error:[[ 1.6950428  -2.5516498   1.9101233 ]\n",
            " [ 1.0878788  -0.09027165  0.29277384]\n",
            " [ 0.57212424 -0.21477687 -1.2889645 ]\n",
            " [ 0.08081627 -0.30215716 -1.3815272 ]\n",
            " [-0.39601374  1.8736382   1.751705  ]]\n",
            "loss:1.9773621559143066\n",
            "---\n",
            "dJ/dW = dJ/dY . dY/dW\n",
            "dJ/dY: [[ 0.07533524 -0.11340666  0.08489437]\n",
            " [ 0.04835017 -0.00401207  0.01301217]\n",
            " [ 0.02542775 -0.00954564 -0.05728731]\n",
            " [ 0.00359183 -0.01342921 -0.06140121]\n",
            " [-0.01760061  0.08327281  0.07785356]]\n",
            "dY/dW: [[ 5.52846     5.52846     5.52846   ]\n",
            " [ 0.69284487  0.69284487  0.69284487]\n",
            " [-2.5651832  -2.5651832  -2.5651832 ]]\n",
            "dJ/dW: [[ 0.01118971  0.17521289  0.2763547 ]\n",
            " [ 0.06538866 -0.07221927  0.66884   ]\n",
            " [ 0.3671855  -0.72179776 -0.37982026]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step by step\n",
        "Ok, now we seem to consistently be getting successful results. Let's work this through in a logical order.\n",
        "\n",
        "First, some notation:\n",
        "* Let $n$ be the number of samples, represented as rows\n",
        "* Let $l \\in 0..L$ refer to a target layer, with layer $0$ representing the input and layer $L$ representing the final output\n",
        "* Let $Z_l$ be the output matrix from layer $l$, with special cases $Z_0 = X$ and $Z_L = \\hat{Y}$\n",
        "* Let $f_l$ be the number of feature columns at layer $l$, with special cases $f_0$ = columns in $X$ and $f_L$ = columns in $Y$\n",
        "* Let $W_l \\in \\mathbb{R}^{f_{l-1}, f_l}$ be the weighs at layer $l$\n",
        "* Let $J \\in \\mathbb{R}^{f_L}$ be the loss function"
      ],
      "metadata": {
        "id": "yXmJ9a3jmjxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single layer network with vector output\n",
        "The weight gradients at the single layer $l=1$ is given by:\n",
        "$$\\frac{\\partial J}{\\partial W_1} = \\frac{\\partial \\hat{Y}}{\\partial W_1} \\cdot \\frac{\\partial J}{\\partial \\hat{Y}} = Z_0^T \\cdot (\\frac{2}{Nf_L}(\\hat{Y}-Y))$$\n",
        "with dimensions:\n",
        "$$(f_0 \\times f_1) = (f_0 \\times n) \\cdot (n \\times f_1)$$"
      ],
      "metadata": {
        "id": "4J1BKYlQqJ2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1) # for consistency\n",
        "tf.random.set_seed(1) # for consistency\n",
        "n = 15\n",
        "fL = 4\n",
        "W = tf.Variable(tf.random.normal(shape=(3,fL)), dtype=tf.float32)\n",
        "Z0 = tf.random.normal(shape=(n,3))\n",
        "Y = tf.random.normal(shape=(n,fL))\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()  # equivalent to tf.reduce_mean((Y_pred - Y)**2, axis=None) = 1/(N * f_L) * sum((Y_pred-Y)**2)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  Y_pred = tf.matmul(Z0, W)\n",
        "  loss = loss_fn(Y, Y_pred)\n",
        "\n",
        "print(f\"Z0:{Z0[:5,:]}\")\n",
        "print(f\"Y_pred:{Y_pred[:5,:]}...\")\n",
        "print(f\"error:{(Y_pred - Y)[:5,:]}...\")\n",
        "print(f\"loss:{loss}\")\n",
        "print(\"---expect---\")\n",
        "dYdW1 = Z0.numpy().T\n",
        "dJdY = 2/(n*fL)*(Y_pred-Y)\n",
        "print(f\"dY/dW1 = Z0.T: {dYdW1[:,:5]}...<more cols>\")\n",
        "print(f\"dJ/dY = 2(n*fL)*error: {dJdY[:5,:]}...\")\n",
        "print(f\"dJ/dW1: {np.matmul(dYdW1, dJdY)}\")\n",
        "print(\"---actual---\")\n",
        "print(f\"dY/dW1: {tape.gradient(Y_pred, W)}  <--- DIFFERENT\")\n",
        "print(f\"dJ/dY: {tape.gradient(loss, Y_pred)[:5,:]}...\")\n",
        "print(f\"dJ/dW1: {tape.gradient(loss, W)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gahqjn0m3Iw",
        "outputId": "37620e60-df58-458c-f286-b8a7a8880b84"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z0:[[ 0.40308788 -1.0880208  -0.06309535]\n",
            " [ 1.3365567   0.7117601  -0.48928645]\n",
            " [-0.7642213  -1.0372486  -1.2519338 ]\n",
            " [ 0.02122428 -0.5513758  -1.7431698 ]\n",
            " [-0.33536094 -1.0426675   1.0091382 ]]\n",
            "Y_pred:[[ 0.92379737  1.7360169   0.09860273 -0.08831309]\n",
            " [-2.070418    1.7202142   0.88154966 -0.9705327 ]\n",
            " [ 2.81064     0.7384437   0.39943358  1.77825   ]\n",
            " [ 1.6248431   1.8298045   1.0509385   1.3398144 ]\n",
            " [ 1.0829754  -0.22253022 -0.85192645 -0.2146242 ]]...\n",
            "error:[[ 1.3808095   2.1428843  -0.629975    0.8046647 ]\n",
            " [-2.3830295   0.72592175  2.665812   -0.44852757]\n",
            " [ 1.8298397   1.4144287  -0.74618113  1.5721774 ]\n",
            " [ 1.8220595   1.2917111   0.2866087   2.1760063 ]\n",
            " [ 0.7468337  -1.7827321  -1.5747099  -1.2901129 ]]...\n",
            "loss:2.4468226432800293\n",
            "---expect---\n",
            "dY/dW1 = Z0.T: [[ 0.40308788  1.3365567  -0.7642213   0.02122428 -0.33536094]\n",
            " [-1.0880208   0.7117601  -1.0372486  -0.5513758  -1.0426675 ]\n",
            " [-0.06309535 -0.48928645 -1.2519338  -1.7431698   1.0091382 ]]...<more cols>\n",
            "dJ/dY = 2(n*fL)*error: [[ 0.04602699  0.07142948 -0.02099917  0.02682216]\n",
            " [-0.07943432  0.02419739  0.08886041 -0.01495092]\n",
            " [ 0.06099466  0.04714763 -0.02487271  0.05240592]\n",
            " [ 0.06073532  0.04305704  0.00955362  0.07253355]\n",
            " [ 0.02489446 -0.05942441 -0.05249033 -0.04300376]]...\n",
            "dJ/dW1: [[-0.3714791   0.61558646  0.273805   -0.3056535 ]\n",
            " [-0.28127775 -0.44332874 -0.0322482  -0.12770541]\n",
            " [-0.36808622 -0.10061519 -0.2518346  -0.63699085]]\n",
            "---actual---\n",
            "dY/dW1: [[ 5.332174  5.332174  5.332174  5.332174]\n",
            " [-5.244204 -5.244204 -5.244204 -5.244204]\n",
            " [-1.422502 -1.422502 -1.422502 -1.422502]]  <--- DIFFERENT\n",
            "dJ/dY: [[ 0.04602699  0.07142948 -0.02099917  0.02682216]\n",
            " [-0.07943432  0.02419739  0.08886041 -0.01495092]\n",
            " [ 0.06099466  0.04714763 -0.02487271  0.05240592]\n",
            " [ 0.06073532  0.04305704  0.00955362  0.07253355]\n",
            " [ 0.02489446 -0.05942441 -0.05249033 -0.04300376]]...\n",
            "dJ/dW1: [[-0.3714791   0.6155865   0.273805   -0.30565354]\n",
            " [-0.28127775 -0.44332874 -0.0322482  -0.12770541]\n",
            " [-0.3680862  -0.10061519 -0.2518346  -0.6369908 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a few things to take away.\n",
        "\n",
        "Firstly, our calculation of the final $\\frac{\\partial J}{\\partial W_1}$ is exactly correct. That's great news.\n",
        "\n",
        "Secondly, we've reconfirmed how the MSE error produces $\\frac{\\partial J}{\\partial \\hat{Y}}$, by way that we get the same result now.\n",
        "\n",
        "Lastly, the gradient tape provides a very different answer to $\\frac{\\partial \\hat{Y}}{\\partial W_1}$. Specifically, it forces the result into the shape of $W_1$. This makes sense if you were wanting to update your $W_1$ based on this gradient, but not if it's intended to be used as part of the chain-rule. In short, the gradient-tape is designed for a very specific purpose, of producing \"gradients\" for the purpose of \"gradient descent/ascent updates\". This means that we cannot depend on it for helping us compute all the partial derivates.\n",
        "\n",
        "A couple more notes:\n",
        "* You can't use the $\\frac{\\partial \\hat{Y}}{\\partial W_1}$ as calculated by the gradient tape in a chain rule calculation. The matrix sizes don't match.\n",
        "* The gradient tape's `jacobian()` function produces a 3D tensor for the same data, which I also don't know how to use."
      ],
      "metadata": {
        "id": "DwOutwk-woC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two-layer network with vector output\n",
        "Here we have:\n",
        "* $\\hat{Y} = Z_2$\n",
        "\n",
        "And so, the weight gradients at the first layer $l=1$ is given by:\n",
        "$$\\frac{\\partial J}{\\partial W_1} = \\frac{\\partial Z_1}{\\partial W_1} \\cdot \\frac{\\partial J}{\\partial \\hat{Y}} \\cdot \\frac{\\partial \\hat{Y}}{\\partial Z_1} = \\frac{\\partial Z_1}{\\partial W_1} \\cdot \\frac{\\partial J}{\\partial Z_2} \\cdot \\frac{\\partial Z_2}{\\partial Z_1} = Z_0^T \\cdot (\\frac{2}{Nf_L}(\\hat{Y}-Y)) \\cdot W_2^T$$\n",
        "with dimensions:\n",
        "$$(f_0 \\times f_1) = (f_0 \\times n) \\cdot (n \\times f_2) \\cdot (f_2 \\times f_1)$$\n",
        "\n",
        "And the weight gradients at the second layer $l=2$ is given by:\n",
        "$$\\frac{\\partial J}{\\partial W_2} = \\frac{\\partial \\hat{Y}}{\\partial W_2} \\cdot \\frac{\\partial J}{\\partial \\hat{Y}} = \\frac{\\partial Z_2}{\\partial W_2} \\cdot \\frac{\\partial J}{\\partial Z_2}= Z_1^T \\cdot (\\frac{2}{Nf_L}(\\hat{Y}-Y))$$\n",
        "with dimensions:\n",
        "$$(f_1 \\times f_2) = (f_1 \\times n) \\cdot (n \\times f_1)$$"
      ],
      "metadata": {
        "id": "N_4_Uik4y7AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1) # for consistency\n",
        "tf.random.set_seed(1) # for consistency\n",
        "n = 15\n",
        "fL = 4\n",
        "Z0 = tf.random.normal(shape=(n,2))\n",
        "W1 = tf.Variable(tf.random.normal(shape=(2,3)), dtype=tf.float32)\n",
        "W2 = tf.Variable(tf.random.normal(shape=(3,fL)), dtype=tf.float32)\n",
        "Y = tf.random.normal(shape=(n,fL))\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()  # equivalent to tf.reduce_mean((Y_pred - Y)**2, axis=None) = 1/(N * f_L) * sum((Y_pred-Y)**2)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  Z1 = tf.matmul(Z0, W1)\n",
        "  Z2 = tf.matmul(Z1, W2)\n",
        "  Y_pred = Z2\n",
        "  loss = loss_fn(Y, Y_pred)\n",
        "\n",
        "print(f\"Z0:{Z0[:5,:]}...\")\n",
        "print(f\"Z1:{Z1[:5,:]}...\")\n",
        "print(f\"Y_pred:{Y_pred[:5,:]}...\")\n",
        "print(f\"error:{(Y_pred - Y)[:5,:]}...\")\n",
        "print(f\"loss:{loss}\")\n",
        "print(\"---expect---\")\n",
        "dZ1dW1 = Z0.numpy().T\n",
        "dZ2dW2 = Z1.numpy().T\n",
        "dZ2dZ1 = W2.numpy().T\n",
        "dJdZ2 = 2/(n*fL)*(Y_pred-Y)\n",
        "print(f\"dZ1/dW1 = Z0.T: {dZ1dW1[:,:5]}...<more cols>\")\n",
        "print(f\"dZ2/dW2 = Z1.T: {dZ2dW2[:,:5]}...<more cols>\")\n",
        "print(f\"dZ2/dZ1 = W1.T: {dZ2dZ1}\")\n",
        "print(f\"dJ/dZ2 = 2(n*fL)*error: {dJdZ2[:5,:]}...\")\n",
        "print(f\"dJ/dW1 = dZ1dW1.dJ/dZ2.dZ2/dZ1: {np.matmul(dZ1dW1, np.matmul(dJdZ2, dZ2dZ1))}\")\n",
        "print(f\"dJ/dW2 = dZ2dW2.dJ/dZ2: {np.matmul(dZ2dW2, dJdZ2)}\")\n",
        "print(\"---actual---\")\n",
        "print(f\"dZ1/dW1: {tape.gradient(Z1, W1)} <-- DIFFERENT as expected\")\n",
        "print(f\"dZ2/dW2: {tape.gradient(Z2, W2)} <-- DIFFERENT as expected\")\n",
        "print(f\"dJ/dZ2: {tape.gradient(loss, Y_pred)[:5,:]}...\")\n",
        "print(f\"dJ/dW1: {tape.gradient(loss, W1)}\")\n",
        "print(f\"dJ/dW2: {tape.gradient(loss, W2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoLixLfMvAnx",
        "outputId": "a93680a3-88e6-419e-e3cc-fbc6b216f095"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z0:[[-1.1012203   1.5457517 ]\n",
            " [ 0.383644   -0.87965786]\n",
            " [-1.2246722  -0.9811211 ]\n",
            " [ 0.08780783 -0.20326038]\n",
            " [-0.5581562  -0.72054404]]...\n",
            "Z1:[[ 1.6220962   2.2983549  -0.6868335 ]\n",
            " [-1.0210704  -1.0435181   0.4061985 ]\n",
            " [-1.8049746   0.634146    0.5573204 ]\n",
            " [-0.23627473 -0.24020937  0.09391228]\n",
            " [-1.1880339   0.09443104  0.38776952]]...\n",
            "Y_pred:[[-0.69647235  2.0895483  -3.7058916  -2.7897866 ]\n",
            " [ 0.5388256  -0.89670616  1.583328    1.5402212 ]\n",
            " [ 1.5697569   0.9881714  -1.8080726   1.3956233 ]\n",
            " [ 0.12499745 -0.20618922  0.36403933  0.3557314 ]\n",
            " [ 0.9527908   0.31513777 -0.5898303   1.0915031 ]]...\n",
            "error:[[-2.390489    1.9698552  -2.5474315  -2.9623907 ]\n",
            " [ 1.2533219  -1.5863068   2.6741438   2.7267315 ]\n",
            " [ 2.871945   -0.23583072 -1.8310329   2.5857396 ]\n",
            " [ 2.5590734   0.83327395 -0.561208    0.10472724]\n",
            " [-0.23693484 -0.1649693  -0.70998156  1.8163954 ]]...\n",
            "loss:3.9742300510406494\n",
            "---expect---\n",
            "dZ1/dW1 = Z0.T: [[-1.1012203   0.383644   -1.2246722   0.08780783 -0.5581562 ]\n",
            " [ 1.5457517  -0.87965786 -0.9811211  -0.20326038 -0.72054404]]...<more cols>\n",
            "dZ2/dW2 = Z1.T: [[ 1.6220962  -1.0210704  -1.8049746  -0.23627473 -1.1880339 ]\n",
            " [ 2.2983549  -1.0435181   0.634146   -0.24020937  0.09443104]\n",
            " [-0.6868335   0.4061985   0.5573204   0.09391228  0.38776952]]...<more cols>\n",
            "dZ2/dZ1 = W1.T: [[-0.45701224  0.31261146  0.98080045]\n",
            " [-0.40686727  0.9942925  -0.6759851 ]\n",
            " [ 0.72857773 -1.7842624   1.1456147 ]\n",
            " [-0.8929778  -0.52200514  0.20607261]]\n",
            "dJ/dZ2 = 2(n*fL)*error: [[-0.07968298  0.06566184 -0.08491439 -0.09874636]\n",
            " [ 0.0417774  -0.0528769   0.08913813  0.09089106]\n",
            " [ 0.0957315  -0.00786102 -0.06103443  0.08619133]\n",
            " [ 0.08530245  0.0277758  -0.01870693  0.00349091]\n",
            " [-0.00789783 -0.00549898 -0.02366605  0.06054652]]...\n",
            "dJ/dW1 = dZ1dW1.dJ/dZ2.dZ2/dZ1: [[ 1.0742333 -2.4325285  1.6147068]\n",
            " [ 0.2615306  1.3745677 -1.2567915]]\n",
            "dJ/dW2 = dZ2dW2.dJ/dZ2: [[-0.49946845  0.04510563 -0.28649634 -0.87503266]\n",
            " [-0.28141594  1.0306022  -1.3993251  -0.36684424]\n",
            " [ 0.183846   -0.08178008  0.18572152  0.31390345]]\n",
            "---actual---\n",
            "dZ1/dW1: [[-0.01669231 -0.01669231 -0.01669231]\n",
            " [-0.377716   -0.377716   -0.377716  ]] <-- DIFFERENT as expected\n",
            "dZ2/dW2: [[-0.5115671  -0.5115671  -0.5115671  -0.5115671 ]\n",
            " [-0.2506814  -0.2506814  -0.2506814  -0.2506814 ]\n",
            " [ 0.18586445  0.18586445  0.18586445  0.18586445]] <-- DIFFERENT as expected\n",
            "dJ/dZ2: [[-0.07968298  0.06566184 -0.08491439 -0.09874636]\n",
            " [ 0.0417774  -0.0528769   0.08913813  0.09089106]\n",
            " [ 0.0957315  -0.00786102 -0.06103443  0.08619133]\n",
            " [ 0.08530245  0.0277758  -0.01870693  0.00349091]\n",
            " [-0.00789783 -0.00549898 -0.02366605  0.06054652]]...\n",
            "dJ/dW1: [[ 1.0742334  -2.4325285   1.6147068 ]\n",
            " [ 0.26153058  1.3745677  -1.2567916 ]]\n",
            "dJ/dW2: [[-0.49946842  0.04510565 -0.28649637 -0.87503266]\n",
            " [-0.28141594  1.0306022  -1.399325   -0.36684427]\n",
            " [ 0.183846   -0.08178008  0.1857215   0.31390348]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! That worked as expected."
      ],
      "metadata": {
        "id": "VRNGl8Ue8l0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single layer network with activation function as diagonal matrix\n",
        "Now we'll introduce activation functions by representing them diagonal matrices $S$ s.t. $S_{ii} = \\sigma_i$ and $\\sigma_i$ is the elementwise scalar multiplicative factor equivalent of the activation function for the given $Z$ input.\n",
        "\n",
        "Here:\n",
        "* $A_0 = Z_0 = X$\n",
        "* $A_L = \\hat{Y}$\n",
        "* $Z_l = X \\cdot W_l$\n",
        "* $A_l = Z_L \\cdot S_l$\n",
        "\n",
        "The weight gradients at the single layer $l=1$ is given by:\n",
        "$$\\frac{\\partial J}{\\partial W_1} = \\frac{\\partial Z_1}{\\partial W_1} \\cdot \\frac{\\partial J}{\\partial (A_1 = \\hat{Y})} \\cdot \\frac{\\partial (A_1 = \\hat{Y})}{\\partial Z_1} = Z_0^T \\cdot (\\frac{2}{Nf_L}(\\hat{Y}-Y)) \\cdot S_l^T$$\n",
        "with dimensions:\n",
        "$$(f_0 \\times f_1) = (f_0 \\times n) \\cdot (n \\times f_1) \\cdot (f_1 \\times f_1)$$"
      ],
      "metadata": {
        "id": "kOPsaEiOJ8Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1) # for consistency\n",
        "tf.random.set_seed(1) # for consistency\n",
        "n = 15\n",
        "fL = 4\n",
        "Z0 = tf.random.normal(shape=(n,3))\n",
        "W1 = tf.Variable(tf.random.normal(shape=(3,fL)), dtype=tf.float32)\n",
        "S1 = tf.cast(tf.linalg.diag(np.random.normal(size=fL)), dtype=tf.float32)\n",
        "Y = tf.random.normal(shape=(n,fL))\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()  # equivalent to tf.reduce_mean((Y_pred - Y)**2, axis=None) = 1/(N * f_L) * sum((Y_pred-Y)**2)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  Z1 = tf.matmul(Z0, W1)\n",
        "  A1 = tf.matmul(Z1, S1)\n",
        "  Y_pred = A1\n",
        "  loss = loss_fn(Y, Y_pred)\n",
        "\n",
        "print(f\"W1:{W1[:5,:]}...\")\n",
        "print(f\"S1:{S1[:5,:]}...\")\n",
        "print(f\"Z0:{Z0[:5,:]}...\")\n",
        "print(f\"Z1:{Z1[:5,:]}...\")\n",
        "print(f\"Y_pred:{Y_pred[:5,:]}...\")\n",
        "print(f\"error:{(Y_pred - Y)[:5,:]}...\")\n",
        "print(f\"loss:{loss}\")\n",
        "print(\"---expect---\")\n",
        "dZ1dW1 = Z0.numpy().T\n",
        "dJdA1 = 2/(n*fL)*(Y_pred-Y)\n",
        "dA1dZ1 = S1.numpy().T\n",
        "print(f\"dZ1/dW1 = Z0.T: {dZ1dW1[:,:5]}...<more cols>\")\n",
        "print(f\"dJ/dA1 = 2(n*fL)*error: {dJdA1[:5,:]}...\")\n",
        "print(f\"dA1/dZ1 = W1.T: {dA1dZ1}\")\n",
        "print(f\"dJ/dW1 = dZ1dW1.dJ/dZ2.dZ2/dZ1: {np.matmul(dZ1dW1, np.matmul(dJdA1, dA1dZ1))}\")\n",
        "print(\"---actual---\")\n",
        "print(f\"dZ1/dW1: {tape.gradient(Z1, W1)} <-- DIFFERENT as expected\")\n",
        "print(f\"dJ/dA1: {tape.gradient(loss, Y_pred)[:5,:]}...\")\n",
        "print(f\"dA1/dZ1: {tape.gradient(A1, Z1)} <-- DIFFERENT as expected\")\n",
        "print(f\"dJ/dW1: {tape.gradient(loss, W1)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEqeLJo_MbQQ",
        "outputId": "9cc0eb51-c8fc-4a55-b20b-7833901a573e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1:[[ 0.40308788 -1.0880208  -0.06309535  1.3365567 ]\n",
            " [ 0.7117601  -0.48928645 -0.7642213  -1.0372486 ]\n",
            " [-1.2519338   0.02122428 -0.5513758  -1.7431698 ]]...\n",
            "S1:[[ 1.6243454  0.         0.         0.       ]\n",
            " [ 0.        -0.6117564  0.         0.       ]\n",
            " [ 0.         0.        -0.5281718  0.       ]\n",
            " [ 0.         0.         0.        -1.0729686]]...\n",
            "Z0:[[-1.1012203   1.5457517   0.383644  ]\n",
            " [-0.87965786 -1.2246722  -0.9811211 ]\n",
            " [ 0.08780783 -0.20326038 -0.5581562 ]\n",
            " [-0.72054404 -0.6259924  -0.71502596]\n",
            " [-0.34835446 -0.33646983  0.18257578]]...\n",
            "Z1:[[ 1.7601889e-01  4.4997773e-01 -1.3233465e+00 -3.7439287e+00]\n",
            " [ 2.0465101e-03  1.5354780e+00  1.5323893e+00  1.8048377e+00]\n",
            " [ 5.8949625e-01 -7.9306550e-03  4.5754948e-01  1.3011527e+00]\n",
            " [ 1.5916619e-01  1.0750805e+00  9.1810775e-01  9.3267345e-01]\n",
            " [-6.0847604e-01  5.4752207e-01  1.7844909e-01 -4.3485320e-01]]...\n",
            "Y_pred:[[ 2.8591549e-01 -2.7527675e-01  6.9895428e-01  4.0171180e+00]\n",
            " [ 3.3242393e-03 -9.3933845e-01 -8.0936480e-01 -1.9365342e+00]\n",
            " [ 9.5754552e-01  4.8516290e-03 -2.4166472e-01 -1.3960960e+00]\n",
            " [ 2.5854087e-01 -6.5768737e-01 -4.8491859e-01 -1.0007293e+00]\n",
            " [-9.8837525e-01 -3.3495012e-01 -9.4251774e-02  4.6658382e-01]]...\n",
            "error:[[ 0.74292773  0.13159052 -0.02962345  4.9100957 ]\n",
            " [-0.30928722 -1.933631    0.9748976  -1.4145291 ]\n",
            " [-0.02325493  0.68083674 -1.3872795  -1.6021686 ]\n",
            " [ 0.45575726 -1.1957808  -1.2492484  -0.16453755]\n",
            " [-1.324517   -1.895152   -0.81703526 -0.60890484]]...\n",
            "loss:2.98830509185791\n",
            "---expect---\n",
            "dZ1/dW1 = Z0.T: [[-1.1012203  -0.87965786  0.08780783 -0.72054404 -0.34835446]\n",
            " [ 1.5457517  -1.2246722  -0.20326038 -0.6259924  -0.33646983]\n",
            " [ 0.383644   -0.9811211  -0.5581562  -0.71502596  0.18257578]]...<more cols>\n",
            "dJ/dA1 = 2(n*fL)*error: [[ 0.02476426  0.00438635 -0.00098745  0.16366987]\n",
            " [-0.01030957 -0.06445437  0.03249659 -0.04715097]\n",
            " [-0.00077516  0.02269456 -0.04624265 -0.05340562]\n",
            " [ 0.01519191 -0.03985936 -0.04164162 -0.00548459]\n",
            " [-0.04415057 -0.06317174 -0.02723451 -0.02029683]]...\n",
            "dA1/dZ1 = W1.T: [[ 1.6243454  0.         0.         0.       ]\n",
            " [ 0.        -0.6117564  0.         0.       ]\n",
            " [ 0.         0.        -0.5281718  0.       ]\n",
            " [ 0.         0.         0.        -1.0729686]]\n",
            "dJ/dW1 = dZ1dW1.dJ/dZ2.dZ2/dZ1: [[ 0.37579522 -0.19047004  0.04006339  0.38605946]\n",
            " [ 0.85414815 -0.27754554 -0.11516027 -0.84166175]\n",
            " [-0.5230817  -0.03756626 -0.05971194 -0.6144448 ]]\n",
            "---actual---\n",
            "dZ1/dW1: [[-5.222547  -5.222547  -5.222547  -5.222547 ]\n",
            " [ 6.6467285  6.6467285  6.6467285  6.6467285]\n",
            " [-2.0650048 -2.0650048 -2.0650048 -2.0650048]] <-- DIFFERENT as expected\n",
            "dJ/dA1: [[ 0.02476426  0.00438635 -0.00098745  0.16366987]\n",
            " [-0.01030957 -0.06445437  0.03249659 -0.04715097]\n",
            " [-0.00077516  0.02269456 -0.04624265 -0.05340562]\n",
            " [ 0.01519191 -0.03985936 -0.04164162 -0.00548459]\n",
            " [-0.04415057 -0.06317174 -0.02723451 -0.02029683]]...\n",
            "dA1/dZ1: [[ 1.6243454 -0.6117564 -0.5281718 -1.0729686]\n",
            " [ 1.6243454 -0.6117564 -0.5281718 -1.0729686]\n",
            " [ 1.6243454 -0.6117564 -0.5281718 -1.0729686]\n",
            " [ 1.6243454 -0.6117564 -0.5281718 -1.0729686]\n",
            " [ 1.6243454 -0.6117564 -0.5281718 -1.0729686]\n",
            " [ 1.6243454 -0.6117564 -0.5281718 -1.0729686]\n",
            " [ 1.6243454 -0.6117564 -0.5281718 -1.0729686]\n",
            " [ 1.6243454 -0.6117564 -0.5281718 -1.0729686]\n",
            " [ 1.6243454 -0.6117564 -0.5281718 -1.0729686]\n",
            " [ 1.6243454 -0.6117564 -0.5281718 -1.0729686]\n",
            " [ 1.6243454 -0.6117564 -0.5281718 -1.0729686]\n",
            " [ 1.6243454 -0.6117564 -0.5281718 -1.0729686]\n",
            " [ 1.6243454 -0.6117564 -0.5281718 -1.0729686]\n",
            " [ 1.6243454 -0.6117564 -0.5281718 -1.0729686]\n",
            " [ 1.6243454 -0.6117564 -0.5281718 -1.0729686]] <-- DIFFERENT as expected\n",
            "dJ/dW1: [[ 0.3757952  -0.19047004  0.0400634   0.38605946]\n",
            " [ 0.85414815 -0.27754554 -0.11516027 -0.84166175]\n",
            " [-0.52308166 -0.03756626 -0.05971193 -0.61444485]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two-layer network with diagonal matrix activation functions\n",
        "That worked. So let's wrap-up with a two-layer network."
      ],
      "metadata": {
        "id": "bo5GoAPBPsRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1) # for consistency\n",
        "tf.random.set_seed(1) # for consistency\n",
        "n = 15\n",
        "fL = 4\n",
        "Z0 = tf.random.normal(shape=(n,2))\n",
        "W1 = tf.Variable(tf.random.normal(shape=(2,3)), dtype=tf.float32)\n",
        "S1 = tf.cast(tf.linalg.diag(np.random.normal(size=3)), dtype=tf.float32)\n",
        "W2 = tf.Variable(tf.random.normal(shape=(3,fL)), dtype=tf.float32)\n",
        "S2 = tf.cast(tf.linalg.diag(np.random.normal(size=fL)), dtype=tf.float32)\n",
        "Y = tf.random.normal(shape=(n,fL))\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()  # equivalent to tf.reduce_mean((Y_pred - Y)**2, axis=None) = 1/(N * f_L) * sum((Y_pred-Y)**2)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  Z1 = tf.matmul(Z0, W1)\n",
        "  A1 = tf.matmul(Z1, S1)\n",
        "  Z2 = tf.matmul(A1, W2)\n",
        "  A2 = tf.matmul(Z2, S2)\n",
        "  Y_pred = A2\n",
        "  loss = loss_fn(Y, Y_pred)\n",
        "\n",
        "print(f\"Z0:{Z0[:5,:]}...\")\n",
        "print(f\"Z1:{Z1[:5,:]}...\")\n",
        "print(f\"Y_pred:{Y_pred[:5,:]}...\")\n",
        "print(f\"error:{(Y_pred - Y)[:5,:]}...\")\n",
        "print(f\"loss:{loss}\")\n",
        "print(\"---expect---\")\n",
        "dZ1dW1 = Z0.numpy().T\n",
        "dZ2dW2 = A1.numpy().T\n",
        "dJdA2 = 2/(n*fL)*(Y_pred-Y)\n",
        "dA2dZ2 = S2.numpy().T\n",
        "dZ2dA1 = W2.numpy().T\n",
        "dA1dZ1 = S1.numpy().T\n",
        "print(f\"dZ1/dW1 = Z0.T: {dZ1dW1[:,:5]}...<more cols>\")\n",
        "print(f\"dJ/dA2 = 2(n*fL)*error: {dJdA2[:5,:]}...\")\n",
        "print(f\"dA2/dZ2 = S2.T: {dA2dZ2}\")\n",
        "print(f\"dZ2/dA1 = W2.T: {dZ2dA1}\")\n",
        "print(f\"dA1/dZ1 = S1.T: {dA1dZ1}\")\n",
        "print(f\"dJ/dW1 = dZ1dW1.dJ/dA2.dA2/dZ2.dZ2dA1.dA1dZ1: {np.matmul(dZ1dW1, np.matmul(dJdA2, np.matmul(np.matmul(dA2dZ2, dZ2dA1), dA1dZ1)))}\")\n",
        "print(f\"dJ/dW2 = dZ2dW2.dJ/dA2.dA2/dZ2: {np.matmul(dZ2dW2, np.matmul(dJdA2, dA2dZ2))}\")\n",
        "print(\"---actual---\")\n",
        "print(f\"dJ/dA2: {tape.gradient(loss, Y_pred)[:5,:]}...\")\n",
        "print(f\"dJ/dW1: {tape.gradient(loss, W1)}\")\n",
        "print(f\"dJ/dW2: {tape.gradient(loss, W2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akiRkh4TP2Rd",
        "outputId": "9a0fd26c-3411-4343-def2-659ed93af47b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z0:[[-1.1012203   1.5457517 ]\n",
            " [ 0.383644   -0.87965786]\n",
            " [-1.2246722  -0.9811211 ]\n",
            " [ 0.08780783 -0.20326038]\n",
            " [-0.5581562  -0.72054404]]...\n",
            "Z1:[[ 1.6220962   2.2983549  -0.6868335 ]\n",
            " [-1.0210704  -1.0435181   0.4061985 ]\n",
            " [-1.8049746   0.634146    0.5573204 ]\n",
            " [-0.23627473 -0.24020937  0.09391228]\n",
            " [-1.1880339   0.09443104  0.38776952]]...\n",
            "Y_pred:[[  1.3818731   -2.3498108  -11.148681    -2.6942422 ]\n",
            " [ -0.8016453    1.2588058    5.968388     1.9256067 ]\n",
            " [ -0.99778616   0.8707299    4.099388     4.815632  ]\n",
            " [ -0.18528685   0.29059806   1.3778011    0.446301  ]\n",
            " [ -0.711374     0.7495918    3.5387316    2.9857194 ]]...\n",
            "error:[[-0.31214356 -2.469504   -9.990221   -2.8668463 ]\n",
            " [-0.08714896  0.56920516  7.059204    3.112117  ]\n",
            " [ 0.30440176 -0.3532722   4.076428    6.005748  ]\n",
            " [ 2.2487893   1.3300612   0.4525537   0.19529685]\n",
            " [-1.9010997   0.26948476  3.4185803   3.7106118 ]]...\n",
            "loss:11.836978912353516\n",
            "---expect---\n",
            "dZ1/dW1 = Z0.T: [[-1.1012203   0.383644   -1.2246722   0.08780783 -0.5581562 ]\n",
            " [ 1.5457517  -0.87965786 -0.9811211  -0.20326038 -0.72054404]]...<more cols>\n",
            "dJ/dA2 = 2(n*fL)*error: [[-0.01040479 -0.08231681 -0.3330074  -0.09556155]\n",
            " [-0.00290497  0.01897351  0.23530681  0.10373724]\n",
            " [ 0.01014673 -0.01177574  0.13588093  0.2001916 ]\n",
            " [ 0.07495965  0.04433538  0.01508512  0.0065099 ]\n",
            " [-0.06336999  0.00898283  0.11395268  0.12368707]]...\n",
            "dA2/dZ2 = S2.T: [[-1.0729686   0.          0.          0.        ]\n",
            " [ 0.          0.86540765  0.          0.        ]\n",
            " [ 0.          0.         -2.3015387   0.        ]\n",
            " [ 0.          0.          0.          1.7448118 ]]\n",
            "dZ2/dA1 = W2.T: [[-0.45701224  0.31261146  0.98080045]\n",
            " [-0.40686727  0.9942925  -0.6759851 ]\n",
            " [ 0.72857773 -1.7842624   1.1456147 ]\n",
            " [-0.8929778  -0.52200514  0.20607261]]\n",
            "dA1/dZ1 = S1.T: [[ 1.6243454  0.         0.       ]\n",
            " [ 0.        -0.6117564  0.       ]\n",
            " [ 0.         0.        -0.5281718]]\n",
            "dJ/dW1 = dZ1dW1.dJ/dA2.dA2/dZ2.dZ2dA1.dA1dZ1: [[-0.94210076 -3.1201982   1.7424324 ]\n",
            " [ 8.965289    5.4942007  -3.029022  ]]\n",
            "dJ/dW2 = dZ2dW2.dJ/dA2.dA2/dZ2: [[-0.5745542  -0.87063134  9.743793   -4.3527937 ]\n",
            " [-0.01501286  0.17844735 -3.9475574  -0.17084217]\n",
            " [-0.06110034 -0.10384967  1.271437   -0.45969614]]\n",
            "---actual---\n",
            "dJ/dA2: [[-0.01040479 -0.08231681 -0.3330074  -0.09556155]\n",
            " [-0.00290497  0.01897351  0.23530681  0.10373724]\n",
            " [ 0.01014673 -0.01177574  0.13588093  0.2001916 ]\n",
            " [ 0.07495965  0.04433538  0.01508512  0.0065099 ]\n",
            " [-0.06336999  0.00898283  0.11395268  0.12368707]]...\n",
            "dJ/dW1: [[-0.9421011 -3.1201985  1.7424322]\n",
            " [ 8.965289   5.494201  -3.0290217]]\n",
            "dJ/dW2: [[-0.57455415 -0.87063134  9.743793   -4.3527937 ]\n",
            " [-0.01501286  0.17844734 -3.9475572  -0.1708422 ]\n",
            " [-0.06110033 -0.10384967  1.271437   -0.45969617]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amazing!! I actually got it to work. Phew!\n",
        "\n",
        "ðŸ˜ŠðŸ‘ŒðŸ™Œ"
      ],
      "metadata": {
        "id": "Fgy2trSJSfVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradients across layers\n",
        "Now that we've got the basics down pat we can start to explore more, and we can do this more efficiently by using TF to do the calculations for us - knowing the limitations of what it's good for and what it's not. Specifically:\n",
        "* TF is good at gradients of the loss w.r.t. variables\n",
        "* TF is not good at gradients of in-between partials\n",
        "\n",
        "First, we'll get a baseline by looking at how the weight gradients change across the layers and with increasing numbers of layers without activation functions. We'll calibrate our networks s.t. the error is $\\frac{1}{nf_L}(\\hat{Y} - Y)$ is $\\vec{1}$ for every row."
      ],
      "metadata": {
        "id": "VdKKJOj0YgsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Representative\n",
        "This shows the full steps for a 2-layer network."
      ],
      "metadata": {
        "id": "Bp22WN0-g2hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 15\n",
        "fL = 4\n",
        "Z0 = tf.constant(np.ones(shape=(n,2)), dtype=tf.float32)\n",
        "W1 = tf.Variable(np.ones(shape=(2,3)), dtype=tf.float32)\n",
        "W2 = tf.Variable(np.ones(shape=(3,fL)), dtype=tf.float32)\n",
        "expected_Y_values = Z0.shape[1] * W1.shape[1]\n",
        "Y = tf.constant(np.ones(shape=(n,fL)) * (expected_Y_values - 1), dtype=tf.float32)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  Z1 = tf.matmul(Z0, W1)\n",
        "  Z2 = tf.matmul(Z1, W2)\n",
        "  Y_pred = Z2\n",
        "  loss = loss_fn(Y, Y_pred)\n",
        "\n",
        "print(f\"Z0:{Z0[:5,:]}...\")\n",
        "print(f\"Z1:{Z1[:5,:]}...\")\n",
        "print(f\"Y_pred:{Y_pred[:5,:]}...\")\n",
        "print(f\"error:{(Y_pred - Y)[:5,:]}...\")\n",
        "print(f\"loss:{loss}\")\n",
        "print(\"---actual---\")\n",
        "print(f\"2/(nfL)*1: {2/(n*fL)}\")\n",
        "print(f\"dJ/dA2: {tape.gradient(loss, Y_pred)[:5,:]}...\")\n",
        "print(f\"dJ/dW1: {tape.gradient(loss, W1)}\")\n",
        "print(f\"dJ/dW2: {tape.gradient(loss, W2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viooEmyLg7qp",
        "outputId": "2f64c0d8-9795-41d7-b63f-078c7ff96097"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z0:[[1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]]...\n",
            "Z1:[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]...\n",
            "Y_pred:[[6. 6. 6. 6.]\n",
            " [6. 6. 6. 6.]\n",
            " [6. 6. 6. 6.]\n",
            " [6. 6. 6. 6.]\n",
            " [6. 6. 6. 6.]]...\n",
            "error:[[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]...\n",
            "loss:1.0\n",
            "---actual---\n",
            "2/(nfL)*1: 0.03333333333333333\n",
            "dJ/dA2: [[0.03333334 0.03333334 0.03333334 0.03333334]\n",
            " [0.03333334 0.03333334 0.03333334 0.03333334]\n",
            " [0.03333334 0.03333334 0.03333334 0.03333334]\n",
            " [0.03333334 0.03333334 0.03333334 0.03333334]\n",
            " [0.03333334 0.03333334 0.03333334 0.03333334]]...\n",
            "dJ/dW1: [[2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "dJ/dW2: [[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Various layered networks without activation function\n",
        "The following shows the gradients for each weight layer, where the error has been calibrated to $\\vec{1}$, with the loss partial as $2/(nf_L)*1$."
      ],
      "metadata": {
        "id": "-RhGs69YfzdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 15\n",
        "f = [3, 3, 3, 3, 3, 3]\n",
        "\n",
        "for L in range(1,len(f)):\n",
        "  print(f\"---{L}-layer network---\")\n",
        "  fL = f[L]\n",
        "  W = [0] * (L+1)\n",
        "  Z = [0] * (L+1)\n",
        "  Z[0] = tf.constant(np.ones(shape=(n,f[0])), dtype=tf.float32)\n",
        "\n",
        "  for l in range(1,L+1):\n",
        "    W[l] = tf.Variable(np.ones(shape=(f[l-1],f[l])), dtype=tf.float32)\n",
        "\n",
        "  expected_Y_values = Z[0].shape[1]\n",
        "  for l in range(1,L):\n",
        "    expected_Y_values *= W[l].shape[1]\n",
        "  expected_error = 1\n",
        "\n",
        "  Y = tf.constant(np.ones(shape=(n,fL)) * (expected_Y_values - expected_error), dtype=tf.float32)\n",
        "  loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    for l in range(1,L+1):\n",
        "      Z[l] = tf.matmul(Z[l-1], W[l])\n",
        "    Y_pred = Z[L]\n",
        "    loss = loss_fn(Y, Y_pred)\n",
        "\n",
        "  #print(f\"(2/nfL)*1: {2/(n*fL)}\")\n",
        "  #print(f\"dJ/dZL: {tape.gradient(loss, Z[L])[:5,:]}...\")\n",
        "  for l in range(1,L+1):\n",
        "    print(f\"dJ/dW{l}: {tape.gradient(loss, W[l])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr4dkTYkd8ul",
        "outputId": "d4f41edf-4ed4-4cda-fda0-d24d60b4ff7c"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---1-layer network---\n",
            "dJ/dW1: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "---2-layer network---\n",
            "dJ/dW1: [[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "dJ/dW2: [[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "---3-layer network---\n",
            "dJ/dW1: [[6.000001 6.000001 6.000001]\n",
            " [6.000001 6.000001 6.000001]\n",
            " [6.000001 6.000001 6.000001]]\n",
            "dJ/dW2: [[6.0000005 6.0000005 6.0000005]\n",
            " [6.0000005 6.0000005 6.0000005]\n",
            " [6.0000005 6.0000005 6.0000005]]\n",
            "dJ/dW3: [[6.0000005 6.0000005 6.0000005]\n",
            " [6.0000005 6.0000005 6.0000005]\n",
            " [6.0000005 6.0000005 6.0000005]]\n",
            "---4-layer network---\n",
            "dJ/dW1: [[18. 18. 18.]\n",
            " [18. 18. 18.]\n",
            " [18. 18. 18.]]\n",
            "dJ/dW2: [[18.000002 18.000002 18.000002]\n",
            " [18.000002 18.000002 18.000002]\n",
            " [18.000002 18.000002 18.000002]]\n",
            "dJ/dW3: [[18.000002 18.000002 18.000002]\n",
            " [18.000002 18.000002 18.000002]\n",
            " [18.000002 18.000002 18.000002]]\n",
            "dJ/dW4: [[18. 18. 18.]\n",
            " [18. 18. 18.]\n",
            " [18. 18. 18.]]\n",
            "---5-layer network---\n",
            "dJ/dW1: [[54. 54. 54.]\n",
            " [54. 54. 54.]\n",
            " [54. 54. 54.]]\n",
            "dJ/dW2: [[54. 54. 54.]\n",
            " [54. 54. 54.]\n",
            " [54. 54. 54.]]\n",
            "dJ/dW3: [[54.000004 54.000004 54.000004]\n",
            " [54.000004 54.000004 54.000004]\n",
            " [54.000004 54.000004 54.000004]]\n",
            "dJ/dW4: [[54. 54. 54.]\n",
            " [54. 54. 54.]\n",
            " [54. 54. 54.]]\n",
            "dJ/dW5: [[54. 54. 54.]\n",
            " [54. 54. 54.]\n",
            " [54. 54. 54.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the component values of the gradients increase rapidly with increased depth. This shows the importance of correctly initialising weights.\n",
        "\n",
        "In this case, with 3x3 weight matrices, their gradient component values follow the formula:\n",
        "$$\\frac{2}{f_L}\\prod_{l=0}^{L-1}f_l$$."
      ],
      "metadata": {
        "id": "qp8W1KdzoYso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try the same thing but with better weight matrice normalisation."
      ],
      "metadata": {
        "id": "nevN6UmfqNER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 15\n",
        "f = [3, 3, 3, 3, 3, 3]\n",
        "\n",
        "for L in range(1,len(f)):\n",
        "  print(f\"---{L}-layer network---\")\n",
        "  fL = f[L]\n",
        "  W = [0] * (L+1)\n",
        "  Z = [0] * (L+1)\n",
        "  Z[0] = tf.constant(np.ones(shape=(n,f[0])), dtype=tf.float32)\n",
        "\n",
        "  for l in range(1,L+1):\n",
        "    W[l] = tf.Variable(np.ones(shape=(f[l-1],f[l])) / f[l], dtype=tf.float32)\n",
        "\n",
        "  Y = tf.constant(np.ones(shape=(n,fL)) - 1, dtype=tf.float32)\n",
        "  loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    for l in range(1,L+1):\n",
        "      Z[l] = tf.matmul(Z[l-1], W[l])\n",
        "    Y_pred = Z[L]\n",
        "    loss = loss_fn(Y, Y_pred)\n",
        "\n",
        "  #print(f\"Y_pred: {Y_pred[:5,:]}...\")\n",
        "  #print(f\"loss: {loss}\")\n",
        "  #print(f\"Y_pred - Y: {(Y_pred - Y)[:5,:]}...\")\n",
        "  #print(f\"(2/nfL)*1: {2/(n*fL)}\")\n",
        "  #print(f\"dJ/dZL: {tape.gradient(loss, Z[L])[:5,:]}...\")\n",
        "  for l in range(1,L+1):\n",
        "    print(f\"dJ/dW{l}: {tape.gradient(loss, W[l])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAfyb6nzhjMJ",
        "outputId": "dbdf5b7b-ec7d-42ba-84e4-102bbc08cdbf"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---1-layer network---\n",
            "dJ/dW1: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "---2-layer network---\n",
            "dJ/dW1: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW2: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "---3-layer network---\n",
            "dJ/dW1: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW2: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW3: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "---4-layer network---\n",
            "dJ/dW1: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW2: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW3: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW4: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "---5-layer network---\n",
            "dJ/dW1: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW2: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW3: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW4: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW5: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great. Now we have a strategy for generating consistent-valued gradients for different network depths.\n",
        "\n",
        "Next, we add activation...."
      ],
      "metadata": {
        "id": "p13v4awHr2NR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Various-layered networks with activation function\n",
        "Just to make sure we can do this, we add a simple $\\mathbb{I}$ activation function at all layers. This is redundant for now but proves out the code."
      ],
      "metadata": {
        "id": "NmMT3xjVtE4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 15\n",
        "f = [3, 3, 3, 3, 3, 3]\n",
        "\n",
        "for L in range(1,len(f)):\n",
        "#for L in range(2,3):\n",
        "  print(f\"---{L}-layer network---\")\n",
        "  fL = f[L]\n",
        "  W = [0] * (L+1)\n",
        "  S = [0] * (L+1)\n",
        "  Z = [0] * (L+1)\n",
        "  A = [0] * (L+1)\n",
        "  A[0] = tf.constant(np.ones(shape=(n,f[0])), dtype=tf.float32)\n",
        "\n",
        "  for l in range(1,L+1):\n",
        "    W[l] = tf.Variable(np.ones(shape=(f[l-1],f[l])) / f[l], dtype=tf.float32)\n",
        "    S[l] = tf.constant(np.eye(f[l]), dtype=tf.float32)\n",
        "\n",
        "  Y = tf.constant(np.ones(shape=(n,fL)) - 1, dtype=tf.float32)\n",
        "  loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    for l in range(1,L+1):\n",
        "      Z[l] = tf.matmul(A[l-1], W[l])\n",
        "      A[l] = tf.matmul(Z[l], S[l])\n",
        "    Y_pred = A[L]\n",
        "    loss = loss_fn(Y, Y_pred)\n",
        "\n",
        "  #print(f\"Y_pred: {Y_pred[:5,:]}...\")\n",
        "  #print(f\"loss: {loss}\")\n",
        "  #print(f\"Y_pred - Y: {(Y_pred - Y)[:5,:]}...\")\n",
        "  #print(f\"(2/nfL)*1: {2/(n*fL)}\")\n",
        "  #print(f\"dJ/dZL: {tape.gradient(loss, Z[L])[:5,:]}...\")\n",
        "  for l in range(1,L+1):\n",
        "    print(f\"dJ/dW{l}: {tape.gradient(loss, W[l])}\")\n",
        "    #print(f\"dJ/dA{l}: {tape.gradient(loss, A[l])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSIqNSG3n71u",
        "outputId": "fc2fc022-d716-4989-d66f-0ae13beff1ac"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---1-layer network---\n",
            "dJ/dW1: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "---2-layer network---\n",
            "dJ/dW1: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW2: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "---3-layer network---\n",
            "dJ/dW1: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW2: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW3: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "---4-layer network---\n",
            "dJ/dW1: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW2: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW3: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW4: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "---5-layer network---\n",
            "dJ/dW1: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW2: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW3: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW4: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n",
            "dJ/dW5: [[0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]\n",
            " [0.6666667 0.6666667 0.6666667]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the real thing.\n",
        "For this, we'll leave the activations as $\\mathbb{I}$ for most layers. We'll attempt to pick a middle layer, and for that one layer we'll zero-out the middle activation value only."
      ],
      "metadata": {
        "id": "LIj96_3MtVzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 15\n",
        "f = [3, 3, 3, 3, 3, 3, 3, 3]\n",
        "\n",
        "for L in range(1,len(f)):\n",
        "#for L in range(2,3):\n",
        "  print(f\"---{L}-layer network---\")\n",
        "  fL = f[L]\n",
        "  W = [0] * (L+1)\n",
        "  S = [0] * (L+1)\n",
        "  Z = [0] * (L+1)\n",
        "  A = [0] * (L+1)\n",
        "  A[0] = tf.constant(np.ones(shape=(n,f[0])), dtype=tf.float32)\n",
        "\n",
        "  for l in range(1,L+1):\n",
        "    W[l] = tf.Variable(np.ones(shape=(f[l-1],f[l])) / f[l], dtype=tf.float32)\n",
        "    S[l] = tf.constant(np.eye(f[l]), dtype=tf.float32)\n",
        "    # zero out middle activation of middle layer\n",
        "    if l == math.ceil(L/2.):\n",
        "      diag = [1.] * f[l]\n",
        "      diag[f[l] // 2] = 0.\n",
        "      S[l] = tf.constant(tf.linalg.diag(diag))\n",
        "      print(f\"S{l}: {S[l]}\")\n",
        "\n",
        "  Y = tf.constant(np.ones(shape=(n,fL)) - 1, dtype=tf.float32)\n",
        "  loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    for l in range(1,L+1):\n",
        "      Z[l] = tf.matmul(A[l-1], W[l])\n",
        "      A[l] = tf.matmul(Z[l], S[l])\n",
        "    Y_pred = A[L]\n",
        "    loss = loss_fn(Y, Y_pred)\n",
        "\n",
        "  #print(f\"Y_pred: {Y_pred[:5,:]}...\")\n",
        "  #print(f\"loss: {loss}\")\n",
        "  #print(f\"Y_pred - Y: {(Y_pred - Y)[:5,:]}...\")\n",
        "  #print(f\"(2/nfL)*1: {2/(n*fL)}\")\n",
        "  #print(f\"dJ/dZL: {tape.gradient(loss, Z[L])[:5,:]}...\")\n",
        "  for l in range(1,L+1):\n",
        "    print(f\"dJ/dW{l}: {tape.gradient(loss, W[l])}\")\n",
        "    #print(f\"dJ/dA{l}: {tape.gradient(loss, A[l])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PQvAnh2sFuM",
        "outputId": "e68c1e89-2357-4c51-c7cf-e60c3bec85b4"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---1-layer network---\n",
            "S1: [[1. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "dJ/dW1: [[0.6666667 0.        0.6666667]\n",
            " [0.6666667 0.        0.6666667]\n",
            " [0.6666667 0.        0.6666667]]\n",
            "---2-layer network---\n",
            "S1: [[1. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "dJ/dW1: [[0.44444445 0.         0.44444445]\n",
            " [0.44444445 0.         0.44444445]\n",
            " [0.44444445 0.         0.44444445]]\n",
            "dJ/dW2: [[0.44444445 0.44444445 0.44444445]\n",
            " [0.         0.         0.        ]\n",
            " [0.44444445 0.44444445 0.44444445]]\n",
            "---3-layer network---\n",
            "S2: [[1. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "dJ/dW1: [[0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]]\n",
            "dJ/dW2: [[0.44444445 0.         0.44444445]\n",
            " [0.44444445 0.         0.44444445]\n",
            " [0.44444445 0.         0.44444445]]\n",
            "dJ/dW3: [[0.44444445 0.44444445 0.44444445]\n",
            " [0.         0.         0.        ]\n",
            " [0.44444445 0.44444445 0.44444445]]\n",
            "---4-layer network---\n",
            "S2: [[1. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "dJ/dW1: [[0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]]\n",
            "dJ/dW2: [[0.44444445 0.         0.44444445]\n",
            " [0.44444445 0.         0.44444445]\n",
            " [0.44444445 0.         0.44444445]]\n",
            "dJ/dW3: [[0.44444445 0.44444445 0.44444445]\n",
            " [0.         0.         0.        ]\n",
            " [0.44444445 0.44444445 0.44444445]]\n",
            "dJ/dW4: [[0.29629633 0.29629633 0.29629633]\n",
            " [0.29629633 0.29629633 0.29629633]\n",
            " [0.29629633 0.29629633 0.29629633]]\n",
            "---5-layer network---\n",
            "S3: [[1. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "dJ/dW1: [[0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]]\n",
            "dJ/dW2: [[0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]]\n",
            "dJ/dW3: [[0.44444445 0.         0.44444445]\n",
            " [0.44444445 0.         0.44444445]\n",
            " [0.44444445 0.         0.44444445]]\n",
            "dJ/dW4: [[0.44444445 0.44444445 0.44444445]\n",
            " [0.         0.         0.        ]\n",
            " [0.44444445 0.44444445 0.44444445]]\n",
            "dJ/dW5: [[0.29629633 0.29629633 0.29629633]\n",
            " [0.29629633 0.29629633 0.29629633]\n",
            " [0.29629633 0.29629633 0.29629633]]\n",
            "---6-layer network---\n",
            "S3: [[1. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "dJ/dW1: [[0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]]\n",
            "dJ/dW2: [[0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]]\n",
            "dJ/dW3: [[0.44444445 0.         0.44444445]\n",
            " [0.44444445 0.         0.44444445]\n",
            " [0.44444445 0.         0.44444445]]\n",
            "dJ/dW4: [[0.44444445 0.44444445 0.44444445]\n",
            " [0.         0.         0.        ]\n",
            " [0.44444445 0.44444445 0.44444445]]\n",
            "dJ/dW5: [[0.29629633 0.29629633 0.29629633]\n",
            " [0.29629633 0.29629633 0.29629633]\n",
            " [0.29629633 0.29629633 0.29629633]]\n",
            "dJ/dW6: [[0.29629633 0.29629633 0.29629633]\n",
            " [0.29629633 0.29629633 0.29629633]\n",
            " [0.29629633 0.29629633 0.29629633]]\n",
            "---7-layer network---\n",
            "S4: [[1. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "dJ/dW1: [[0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]]\n",
            "dJ/dW2: [[0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]]\n",
            "dJ/dW3: [[0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]\n",
            " [0.2962963 0.2962963 0.2962963]]\n",
            "dJ/dW4: [[0.44444445 0.         0.44444445]\n",
            " [0.44444445 0.         0.44444445]\n",
            " [0.44444445 0.         0.44444445]]\n",
            "dJ/dW5: [[0.44444445 0.44444445 0.44444445]\n",
            " [0.         0.         0.        ]\n",
            " [0.44444445 0.44444445 0.44444445]]\n",
            "dJ/dW6: [[0.29629633 0.29629633 0.29629633]\n",
            " [0.29629633 0.29629633 0.29629633]\n",
            " [0.29629633 0.29629633 0.29629633]]\n",
            "dJ/dW7: [[0.29629633 0.29629633 0.29629633]\n",
            " [0.29629633 0.29629633 0.29629633]\n",
            " [0.29629633 0.29629633 0.29629633]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ouTm-H30uOty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-MAIu6-LvclH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}